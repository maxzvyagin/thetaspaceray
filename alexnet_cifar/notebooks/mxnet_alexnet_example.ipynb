{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://gluon.mxnet.io/chapter04_convolutional-neural-networks/deep-cnns-alexnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mxnet.gluon import HybridBlock, nn\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctx = mx.gpu()\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(data, label):\n",
    "    data = mx.image.imresize(data, 224, 224)\n",
    "    #data = mx.nd.transpose(data, (2,0,1))\n",
    "    data = data.astype(np.float32)\n",
    "    return data, label\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10('./data', train=True, transform=transformer),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard')\n",
    "\n",
    "test_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10('./data', train=False, transform=transformer),\n",
    "    batch_size=batch_size, shuffle=False, last_batch='discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net = gluon.nn.Sequential()\n",
    "with alex_net.name_scope():\n",
    "    #  First convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=96, kernel_size=11, strides=(4,4), activation='relu'))\n",
    "    alex_net.add(gluon.nn.MaxPool2D(pool_size=3, strides=2))\n",
    "    #  Second convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=192, kernel_size=5, activation='relu'))\n",
    "    alex_net.add(gluon.nn.MaxPool2D(pool_size=3, strides=(2,2)))\n",
    "    # Third convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=384, kernel_size=3, activation='relu'))\n",
    "    # Fourth convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=384, kernel_size=3, activation='relu'))\n",
    "    # Fifth convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=256, kernel_size=3, activation='relu'))\n",
    "    alex_net.add(gluon.nn.MaxPool2D(pool_size=3, strides=2))\n",
    "    # Flatten and apply fullly connected layers\n",
    "    alex_net.add(gluon.nn.Flatten())\n",
    "    alex_net.add(gluon.nn.Dense(4096, activation=\"relu\"))\n",
    "    alex_net.add(gluon.nn.Dense(4096, activation=\"relu\"))\n",
    "    alex_net.add(gluon.nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(HybridBlock):\n",
    "    r\"\"\"AlexNet model from the `\"One weird trick...\" `_ paper.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    classes : int, default 1000\n",
    "        Number of classes for the output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, classes=1000, **kwargs):\n",
    "        super(AlexNet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            with self.features.name_scope():\n",
    "                self.features.add(nn.Conv2D(64, kernel_size=11, strides=4,\n",
    "                                            padding=5, activation='relu', in_channels=3,layout=\"NHWC\"))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "                self.features.add(nn.Conv2D(256, kernel_size=5, padding=2,\n",
    "                                            activation='relu',layout=\"NHWC\"))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "                self.features.add(nn.Conv2D(384, kernel_size=3, padding=1,\n",
    "                                            activation='relu', layout=\"NHWC\"))\n",
    "                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1,\n",
    "                                            activation='relu', layout=\"NHWC\"))\n",
    "                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1,\n",
    "                                            activation='relu', layout=\"NHWC\"))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "                self.features.add(nn.Flatten())\n",
    "                self.features.add(nn.Dense(4096, activation='relu'))\n",
    "                self.features.add(nn.Dropout(0.5))\n",
    "                self.features.add(nn.Dense(4096, activation='relu'))\n",
    "                self.features.add(nn.Dropout(0.5))\n",
    "\n",
    "            self.output = nn.Dense(classes)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.features(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net = AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(alex_net.collect_params(), mx.optimizer.Adam(learning_rate=.001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for d, l in data_iterator:\n",
    "        data = d.as_in_context(ctx)\n",
    "        label = l.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "MXNetError: could not create a descriptor for a dilated convolution forward propagation primitive",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-bc118067b362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#  Keep a moving average of the losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m##########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mcurr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         moving_loss = (curr_loss if ((i == 0) and (e == 0))\n\u001b[1;32m     23\u001b[0m                        else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2583\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2586\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2561\u001b[0m         \"\"\"\n\u001b[1;32m   2562\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2563\u001b[0;31m         check_call(_LIB.MXNDArraySyncCopyToCPU(\n\u001b[0m\u001b[1;32m   2564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2565\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: MXNetError: could not create a descriptor for a dilated convolution forward propagation primitive"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################\n",
    "#  Only one epoch so tests can run quickly, increase this variable to actually run\n",
    "###########################\n",
    "epochs = 1\n",
    "smoothing_constant = .01\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (d, l) in tqdm(enumerate(train_data)):\n",
    "        data = d.as_in_context(ctx)\n",
    "        label = l.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = alex_net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, alex_net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, alex_net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
