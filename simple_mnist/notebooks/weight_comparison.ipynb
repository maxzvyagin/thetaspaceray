{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Distribution Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import log2\n",
    "import pickle\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'learning_rate': .001, 'dropout': 0.2, 'batch_size': 64, 'epochs': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_weights(config):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(config['dropout']),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    res = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'])\n",
    "    res_test = model.evaluate(x_test, y_test)\n",
    "    just_tf_weights = list()\n",
    "    # get weights\n",
    "    for w in model.weights:\n",
    "        just_tf_weights.extend(w.numpy().flatten())\n",
    "    # scale the weights\n",
    "    scaled_weights = MinMaxScaler().fit_transform(np.array(just_tf_weights).reshape(-1, 1))+1\n",
    "    return scaled_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNet(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(784, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(config['dropout']), \n",
    "            nn.Linear(128, 10)) ### no softmax because it's included in cross entropy loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.config = config\n",
    "        self.test_loss = None\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'test_loss': loss}\n",
    "        return {'test_loss': loss, 'logs': logs}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = []\n",
    "        for x in outputs:\n",
    "            loss.append(float(x['test_loss']))\n",
    "        avg_loss = statistics.mean(loss)\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        self.test_loss = avg_loss\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pt_weights(config):\n",
    "    model = NumberNet(config)\n",
    "    trainer = pl.Trainer(max_epochs=config['epochs'])\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "    pt_model_weights = list(model.parameters())\n",
    "    just_pt_weights = list()\n",
    "    for w in pt_model_weights:\n",
    "        just_pt_weights.extend(w.detach().numpy().flatten())\n",
    "    pt_weights_scaled = MinMaxScaler().fit_transform(np.array(just_pt_weights).reshape(-1, 1))+1\n",
    "    return pt_weights_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tf_weights = list()\n",
    "for i in range(10):\n",
    "    all_tf_weights.append(get_tf_weights(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tf_weights = np.mean(all_tf_weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_weights = list()\n",
    "for i in range(10):\n",
    "    all_pt_weights.append(get_pt_weights(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pt_weights = np.mean(all_pt_weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3885.5615], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(mean_tf_weights, mean_pt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.856396"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.special.kl_div(mean_tf_weights, mean_pt_weights).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00024667307"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.entropy(mean_tf_weights.flatten(), qk=mean_pt_weights.flatten(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007853274003175311"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.jensenshannon(mean_tf_weights.flatten(), mean_pt_weights.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006773344657721863"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.jensenshannon(np.mean(all_tf_weights, axis=1).flatten(), np.mean(all_pt_weights, axis=1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the top 5 and bottom 5 weight lists from the hyperspace search\n",
    "top_5_tf = pickle.load(open(\"../top_5_config_tf_model_weights.pkl\", \"rb\"))\n",
    "top_5_pt = pickle.load(open(\"../top_5_config_pt_model_weights.pkl\", \"rb\"))\n",
    "bottom_5_tf = pickle.load(open(\"../bottom_5_config_tf_model_weights.pkl\", \"rb\"))\n",
    "bottom_5_pt = pickle.load(open(\"../bottom_5_config_pt_model_weights.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf = np.array(list(top_5_tf.values())[0])\n",
    "test_tf = MinMaxScaler().fit_transform(test_tf.reshape(-1, 1))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pt = np.array(list(top_5_pt.values())[0])\n",
    "test_pt = MinMaxScaler().fit_transform(test_pt.reshape(-1, 1))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1687.7921], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(test_tf, test_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_normalize(a):\n",
    "    # take a list a, return an array normalized between 1 and 2\n",
    "    new_a = MinMaxScaler().fit_transform(np.array(a).reshape(-1, 1))+1\n",
    "    return new_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare each top 5 pt to each top 5 tf\n",
    "top5_euclidean = []\n",
    "for pair in product(top_5_tf, top_5_pt):\n",
    "    tf = top_5_tf[pair[0]]\n",
    "    pt = top_5_pt[pair[1]]\n",
    "#     normal_tf = my_normalize(tf)\n",
    "#     normal_pt = my_normalize(pt)\n",
    "#     top_5_both_jensenshannon.append(scipy.spatial.distance.jensenshannon(tf, pt))\n",
    "#     top_5_both_entropy.append(scipy.stats.entropy(tf, pt))\n",
    "#     top_5_both_mykl.append(kl_divergence(normal_tf, normal_pt))\n",
    "    top5_euclidean.append(scipy.spatial.distance.euclidean(tf, pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix1_euclidean = []\n",
    "for pair in product(top_5_tf, bottom_5_pt):\n",
    "    tf = top_5_tf[pair[0]]\n",
    "    pt = bottom_5_pt[pair[1]]\n",
    "#     normal_tf = my_normalize(tf)\n",
    "#     normal_pt = my_normalize(pt)\n",
    "#     top_5_both_jensenshannon.append(scipy.spatial.distance.jensenshannon(tf, pt))\n",
    "#     top_5_both_entropy.append(scipy.stats.entropy(tf, pt))\n",
    "#     top_5_both_mykl.append(kl_divergence(normal_tf, normal_pt))\n",
    "    mix1_euclidean.append(scipy.spatial.distance.euclidean(tf, pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix2_euclidean = []\n",
    "for pair in product(bottom_5_tf, top_5_pt):\n",
    "    tf = bottom_5_tf[pair[0]]\n",
    "    pt = top_5_pt[pair[1]]\n",
    "    mix2_euclidean.append(scipy.spatial.distance.euclidean(tf, pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom5_euclidean = []\n",
    "for pair in product(bottom_5_tf, bottom_5_pt):\n",
    "    tf = bottom_5_tf[pair[0]]\n",
    "    pt = bottom_5_pt[pair[1]]\n",
    "    bottom5_euclidean.append(scipy.spatial.distance.euclidean(tf, pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[248.22796630859375,\n",
       " 231.326904296875,\n",
       " 220.34353637695312,\n",
       " 272.3984680175781,\n",
       " 157.28028869628906,\n",
       " 259.25592041015625,\n",
       " 242.80320739746094,\n",
       " 232.80638122558594,\n",
       " 281.92523193359375,\n",
       " 174.42401123046875,\n",
       " 204.72842407226562,\n",
       " 183.0611114501953,\n",
       " 168.662109375,\n",
       " 233.46591186523438,\n",
       " 66.2406234741211,\n",
       " 283.1245422363281,\n",
       " 268.6000671386719,\n",
       " 259.2295837402344,\n",
       " 304.4197998046875,\n",
       " 208.2905731201172,\n",
       " 261.2315979003906,\n",
       " 245.09693908691406,\n",
       " 234.21742248535156,\n",
       " 284.2892150878906,\n",
       " 177.13597106933594]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220.34353637695312"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.euclidean(top_5_tf['0.008332646839818986lr_0.3985947224532251drop_64epochs_950batch'], top_5_pt['0.008332646839818986lr_0.3985947224532251drop_64epochs_950batch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233.34609985351562"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.euclidean(bottom_5_tf['0.09949307671494452lr_0.8847296070468049drop_68epochs_67batch'], bottom_5_pt['0.09949307671494452lr_0.8847296070468049drop_68epochs_67batch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250.7360382080078"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.euclidean(top_5_tf['0.008332646839818986lr_0.3985947224532251drop_64epochs_950batch'], bottom_5_pt['0.09949307671494452lr_0.8847296070468049drop_68epochs_67batch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219.71839904785156"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.euclidean(bottom_5_tf['0.09949307671494452lr_0.8847296070468049drop_68epochs_67batch'],  top_5_pt['0.008332646839818986lr_0.3985947224532251drop_64epochs_950batch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228.1034323120117\n",
      "257.1180706787109\n",
      "255.5523388671875\n",
      "265.99940185546876\n"
     ]
    }
   ],
   "source": [
    "for l in [top5_euclidean, mix1_euclidean, mix2_euclidean, bottom5_euclidean]:\n",
    "    print(np.array(l).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248.22796630859375"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.euclidean(list(top_5_tf.values())[0], list(top_5_pt.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250.7360382080078"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.euclidean(list(top_5_tf.values())[0], list(bottom_5_pt.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257.3309020996094"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.euclidean(list(bottom_5_tf.values())[0], list(top_5_pt.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245.04383850097656"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.spatial.distance.euclidean(list(bottom_5_tf.values())[0], list(bottom_5_pt.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221.55516052246094\n"
     ]
    }
   ],
   "source": [
    "### What about comparing within the frameworks, not cross?\n",
    "print(scipy.spatial.distance.euclidean(list(top_5_tf.values())[0], list(bottom_5_tf.values())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274.5293273925781\n"
     ]
    }
   ],
   "source": [
    "print(scipy.spatial.distance.euclidean(list(top_5_pt.values())[0], list(bottom_5_pt.values())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# compare each bottom 5 pt to each bottom 5 tf'\n",
    "bottom_5_both_jensenshannon = []\n",
    "bottom_5_both_entropy = []\n",
    "for pair in product(bottom_5_tf, bottom_5_pt):\n",
    "    bottom_5_both_jensenshannon.append(scipy.spatial.distance.jensenshannon(bottom_5_tf[pair[0]], bottom_5_pt[pair[1]]))\n",
    "    bottom_5_both_entropy.append(scipy.stats.entropy(bottom_5_tf[pair[0]], bottom_5_pt[pair[1]]))\n",
    "    print(scipy.special.kl_div(bottom_5_tf[pair[0]], bottom_5_pt[pair[1]]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# compare each top 5 pt to each bottom 5 tf\n",
    "bottom_5_both_jensenshannon = []\n",
    "bottom_5_both_entropy = []\n",
    "for pair in product(top_5_tf, bottom_5_pt):\n",
    "    bottom_5_both_jensenshannon.append(scipy.spatial.distance.jensenshannon(top_5_tf[pair[0]], bottom_5_pt[pair[1]]))\n",
    "    bottom_5_both_entropy.append(scipy.stats.entropy(top_5_tf[pair[0]], bottom_5_pt[pair[1]]))\n",
    "    print(scipy.special.kl_div(top_5_tf[pair[0]], bottom_5_pt[pair[1]]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_5_both_jensenshannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare each bottom 5 pt to each bottom 5 tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tf_weights, top_pt_weights = top_5_tf['0.008332646839818986lr_0.3985947224532251drop_64epochs_950batch'], top_5_pt['0.008332646839818986lr_0.3985947224532251drop_64epochs_950batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_tf_weights, bottom_pt_weights = bottom_5_tf['0.09949307671494452lr_0.8847296070468049drop_68epochs_67batch'], bottom_5_pt['0.09949307671494452lr_0.8847296070468049drop_68epochs_67batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
