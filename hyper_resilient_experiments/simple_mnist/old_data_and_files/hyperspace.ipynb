{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out Hyperspace on MNIST Data - Model Resiliency Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from hyperspace import create_hyperspace\n",
    "from ray import tune\n",
    "import tensorflow as tf\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from skopt import Optimizer\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import statistics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-09 10:25:13,294\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.240',\n",
       " 'raylet_ip_address': '192.168.1.240',\n",
       " 'redis_address': '192.168.1.240:63779',\n",
       " 'object_store_address': '/tmp/ray/session_2020-10-09_10-25-12_693516_81011/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-10-09_10-25-12_693516_81011/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-10-09_10-25-12_693516_81011',\n",
       " 'metrics_export_port': 60740}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Model Objective Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_tf_objective(config):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(config['dropout']),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    res = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'])\n",
    "    res_test = model.evaluate(x_test, y_test)\n",
    "    # res test[0] reports the loss from the evaluation, res_test[1] reports the accuracy\n",
    "    tune.report(test_loss = res_test[0])\n",
    "    return res_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the hyperspace\n",
    "hyperparameters = [(0.00001, 0.1),  # learning_rate\n",
    "                   (0.2, 0.9),  # dropout\n",
    "                   (10, 100),  # epochs \n",
    "                   (10, 1000)]  # batch size\n",
    "space = create_hyperspace(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Space([Real(low=0.03750625, high=0.1, prior='uniform', transform='identity'),\n",
       "        Real(low=0.4625, high=0.9, prior='uniform', transform='identity'),\n",
       "        Integer(low=44, high=100),\n",
       "        Integer(low=382, high=1000)]),\n",
       " Space([Real(low=1e-05, high=0.06250375000000001, prior='uniform', transform='identity'),\n",
       "        Real(low=0.4625, high=0.9, prior='uniform', transform='identity'),\n",
       "        Integer(low=44, high=100),\n",
       "        Integer(low=382, high=1000)]),\n",
       " Space([Real(low=0.03750625, high=0.1, prior='uniform', transform='identity'),\n",
       "        Real(low=0.2, high=0.6375000000000001, prior='uniform', transform='identity'),\n",
       "        Integer(low=44, high=100),\n",
       "        Integer(low=382, high=1000)]),\n",
       " Space([Real(low=1e-05, high=0.06250375000000001, prior='uniform', transform='identity'),\n",
       "        Real(low=0.2, high=0.6375000000000001, prior='uniform', transform='identity'),\n",
       "        Integer(low=44, high=100),\n",
       "        Integer(low=382, high=1000)]),\n",
       " Space([Real(low=0.03750625, high=0.1, prior='uniform', transform='identity'),\n",
       "        Real(low=0.4625, high=0.9, prior='uniform', transform='identity'),\n",
       "        Integer(low=10, high=66),\n",
       "        Integer(low=382, high=1000)]),\n",
       " Space([Real(low=1e-05, high=0.06250375000000001, prior='uniform', transform='identity'),\n",
       "        Real(low=0.4625, high=0.9, prior='uniform', transform='identity'),\n",
       "        Integer(low=10, high=66),\n",
       "        Integer(low=382, high=1000)]),\n",
       " Space([Real(low=0.03750625, high=0.1, prior='uniform', transform='identity'),\n",
       "        Real(low=0.2, high=0.6375000000000001, prior='uniform', transform='identity'),\n",
       "        Integer(low=10, high=66),\n",
       "        Integer(low=382, high=1000)]),\n",
       " Space([Real(low=1e-05, high=0.06250375000000001, prior='uniform', transform='identity'),\n",
       "        Real(low=0.2, high=0.6375000000000001, prior='uniform', transform='identity'),\n",
       "        Integer(low=10, high=66),\n",
       "        Integer(low=382, high=1000)]),\n",
       " Space([Real(low=0.03750625, high=0.1, prior='uniform', transform='identity'),\n",
       "        Real(low=0.4625, high=0.9, prior='uniform', transform='identity'),\n",
       "        Integer(low=44, high=100),\n",
       "        Integer(low=10, high=628)]),\n",
       " Space([Real(low=1e-05, high=0.06250375000000001, prior='uniform', transform='identity'),\n",
       "        Real(low=0.4625, high=0.9, prior='uniform', transform='identity'),\n",
       "        Integer(low=44, high=100),\n",
       "        Integer(low=10, high=628)]),\n",
       " Space([Real(low=0.03750625, high=0.1, prior='uniform', transform='identity'),\n",
       "        Real(low=0.2, high=0.6375000000000001, prior='uniform', transform='identity'),\n",
       "        Integer(low=44, high=100),\n",
       "        Integer(low=10, high=628)]),\n",
       " Space([Real(low=1e-05, high=0.06250375000000001, prior='uniform', transform='identity'),\n",
       "        Real(low=0.2, high=0.6375000000000001, prior='uniform', transform='identity'),\n",
       "        Integer(low=44, high=100),\n",
       "        Integer(low=10, high=628)]),\n",
       " Space([Real(low=0.03750625, high=0.1, prior='uniform', transform='identity'),\n",
       "        Real(low=0.4625, high=0.9, prior='uniform', transform='identity'),\n",
       "        Integer(low=10, high=66),\n",
       "        Integer(low=10, high=628)]),\n",
       " Space([Real(low=1e-05, high=0.06250375000000001, prior='uniform', transform='identity'),\n",
       "        Real(low=0.4625, high=0.9, prior='uniform', transform='identity'),\n",
       "        Integer(low=10, high=66),\n",
       "        Integer(low=10, high=628)]),\n",
       " Space([Real(low=0.03750625, high=0.1, prior='uniform', transform='identity'),\n",
       "        Real(low=0.2, high=0.6375000000000001, prior='uniform', transform='identity'),\n",
       "        Integer(low=10, high=66),\n",
       "        Integer(low=10, high=628)]),\n",
       " Space([Real(low=1e-05, high=0.06250375000000001, prior='uniform', transform='identity'),\n",
       "        Real(low=0.2, high=0.6375000000000001, prior='uniform', transform='identity'),\n",
       "        Integer(low=10, high=66),\n",
       "        Integer(low=10, high=628)])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run hypertune for Tensorflow Model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-05 14:29:09,570\tWARNING function_runner.py:485 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "2020-10-05 14:30:50,375\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffffbd5c534001000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 14:30:53,524\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:30:53,529\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:30:53,530\tINFO (unknown file):0 -- gc.collect() freed 109 refs in 3.0544720499999585 seconds\n",
      "2020-10-05 14:32:50,119\tWARNING util.py:136 -- The `process_trial` operation took 6.179466009140015 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:32:56,247\tWARNING util.py:136 -- The `process_trial` operation took 5.4819016456604 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:33:02,248\tWARNING util.py:136 -- The `process_trial` operation took 5.925668001174927 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:33:21,302\tWARNING util.py:136 -- The `process_trial` operation took 4.810906171798706 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:33:51,886\tWARNING util.py:136 -- The `process_trial` operation took 7.052622079849243 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:33:58,070\tWARNING util.py:136 -- The `process_trial` operation took 6.0086469650268555 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:33:58,807\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.7123377323150635 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:34:05,223\tWARNING util.py:136 -- The `process_trial` operation took 6.112133264541626 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:34:46,609\tWARNING util.py:136 -- The `process_trial` operation took 3.5007951259613037 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:34:48,347\tWARNING util.py:136 -- The `process_trial` operation took 1.4747350215911865 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:34:50,070\tWARNING util.py:136 -- The `process_trial` operation took 0.9456830024719238 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:34:52,879\tWARNING util.py:136 -- The `process_trial` operation took 0.5308287143707275 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:39:15,271\tWARNING util.py:136 -- The `process_trial` operation took 6.3505449295043945 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:39:26,922\tWARNING util.py:136 -- The `process_trial` operation took 4.607467889785767 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:39:31,664\tWARNING util.py:136 -- The `process_trial` operation took 3.2550320625305176 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:39:35,024\tWARNING util.py:136 -- The `process_trial` operation took 3.231597900390625 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:39:37,637\tWARNING util.py:136 -- The `process_trial` operation took 2.1061110496520996 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:39:40,489\tWARNING util.py:136 -- The `process_trial` operation took 1.5109481811523438 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:39:53,397\tWARNING util.py:136 -- The `process_trial` operation took 4.04427695274353 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:40:28,372\tWARNING util.py:136 -- The `process_trial` operation took 4.440252065658569 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:40:43,000\tWARNING util.py:136 -- The `process_trial` operation took 1.3626301288604736 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:40:43,641\tWARNING util.py:136 -- The `process_trial` operation took 0.5554277896881104 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:40:44,219\tWARNING util.py:136 -- The `process_trial` operation took 0.5290570259094238 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:44:24,843\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff4a4d674101000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 14:44:31,861\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:44:31,867\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:44:31,871\tINFO (unknown file):0 -- gc.collect() freed 435 refs in 6.933784093999975 seconds\n",
      "2020-10-05 14:44:31,876\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:44:31,888\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:45:18,860\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5547699928283691 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:45:25,696\tWARNING util.py:136 -- The `process_trial` operation took 6.795925140380859 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:45:32,907\tWARNING util.py:136 -- The `process_trial` operation took 6.787137985229492 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:45:39,051\tWARNING util.py:136 -- The `process_trial` operation took 5.3998122215271 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:45:42,945\tWARNING util.py:136 -- The `process_trial` operation took 3.5778069496154785 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:45:44,920\tWARNING util.py:136 -- The `process_trial` operation took 1.6145200729370117 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:45:46,623\tWARNING util.py:136 -- The `process_trial` operation took 1.6834158897399902 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:45:48,527\tWARNING util.py:136 -- The `process_trial` operation took 1.8942749500274658 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:46:31,441\tWARNING util.py:136 -- The `process_trial` operation took 5.94392204284668 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:47:04,899\tWARNING util.py:136 -- The `process_trial` operation took 2.206291913986206 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:47:07,545\tWARNING util.py:136 -- The `process_trial` operation took 1.472991943359375 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:47:14,037\tWARNING util.py:136 -- The `process_trial` operation took 0.6765460968017578 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:50:02,170\tWARNING util.py:136 -- The `process_trial` operation took 3.8063228130340576 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:52:21,779\tWARNING util.py:136 -- The `process_trial` operation took 7.007097005844116 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:52:26,355\tWARNING util.py:136 -- The `process_trial` operation took 3.7763519287109375 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:52:29,515\tWARNING util.py:136 -- The `process_trial` operation took 3.0926320552825928 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:52:32,125\tWARNING util.py:136 -- The `process_trial` operation took 2.249293088912964 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:52:34,094\tWARNING util.py:136 -- The `process_trial` operation took 1.6877918243408203 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:52:35,158\tWARNING util.py:136 -- The `process_trial` operation took 0.9800667762756348 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:53:39,438\tWARNING util.py:136 -- The `process_trial` operation took 4.954971790313721 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:53:44,531\tWARNING util.py:136 -- The `process_trial` operation took 2.1413841247558594 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:53:47,269\tWARNING util.py:136 -- The `process_trial` operation took 2.2203190326690674 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:53:51,128\tWARNING util.py:136 -- The `process_trial` operation took 0.8072948455810547 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:56:43,102\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:56:43,108\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:56:43,115\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 14:56:46,599\tWARNING util.py:136 -- The `process_trial` operation took 12.511560201644897 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:56:52,659\tWARNING util.py:136 -- The `process_trial` operation took 5.304680347442627 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:56:59,695\tWARNING util.py:136 -- The `process_trial` operation took 6.822763919830322 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:57:07,224\tWARNING util.py:136 -- The `process_trial` operation took 2.1950740814208984 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:57:09,507\tWARNING util.py:136 -- The `process_trial` operation took 2.0111920833587646 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:57:12,855\tWARNING util.py:136 -- The `process_trial` operation took 3.2267189025878906 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:57:34,252\tWARNING util.py:136 -- The `process_trial` operation took 6.393755912780762 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:57:46,293\tWARNING util.py:136 -- The `process_trial` operation took 4.497071027755737 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:57:49,432\tWARNING util.py:136 -- The `process_trial` operation took 3.022346019744873 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:57:51,656\tWARNING util.py:136 -- The `process_trial` operation took 0.8943500518798828 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 14:59:54,319\tWARNING util.py:136 -- The `process_trial` operation took 4.390354156494141 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:00:36,506\tWARNING util.py:136 -- The `process_trial` operation took 10.411448240280151 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:00:37,016\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5082478523254395 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:00:48,292\tWARNING util.py:136 -- The `process_trial` operation took 6.977349042892456 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:00:48,816\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5216376781463623 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:04,753\tWARNING util.py:136 -- The `process_trial` operation took 7.599950075149536 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:05,273\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5149750709533691 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:12,503\tWARNING util.py:136 -- The `process_trial` operation took 7.0841310024261475 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:23,813\tWARNING util.py:136 -- The `process_trial` operation took 1.8829779624938965 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:26,256\tWARNING util.py:136 -- The `process_trial` operation took 2.3701751232147217 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:29,947\tWARNING util.py:136 -- The `process_trial` operation took 3.6686620712280273 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:37,765\tWARNING util.py:136 -- The `process_trial` operation took 1.4568581581115723 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:39,151\tWARNING util.py:136 -- The `process_trial` operation took 1.3099610805511475 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:01:51,397\tWARNING util.py:136 -- The `process_trial` operation took 0.544213056564331 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:03:51,268\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff45aa236601000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 15:03:52,497\tINFO (unknown file):0 -- gc.collect() freed 275 refs in 1.126252144999853 seconds\n",
      "2020-10-05 15:04:42,626\tWARNING util.py:136 -- The `process_trial` operation took 8.473507165908813 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:04:54,059\tWARNING util.py:136 -- The `process_trial` operation took 4.061168909072876 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:04:58,845\tWARNING util.py:136 -- The `process_trial` operation took 4.340741872787476 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:05:02,381\tWARNING util.py:136 -- The `process_trial` operation took 3.335792064666748 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:05:05,472\tWARNING util.py:136 -- The `process_trial` operation took 2.9550750255584717 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:05:09,406\tWARNING util.py:136 -- The `process_trial` operation took 3.4761929512023926 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:05:43,014\tWARNING util.py:136 -- The `process_trial` operation took 5.221549034118652 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:05:46,417\tWARNING util.py:136 -- The `process_trial` operation took 3.079345941543579 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:06:03,027\tWARNING util.py:136 -- The `process_trial` operation took 1.8179199695587158 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:06:05,630\tWARNING util.py:136 -- The `process_trial` operation took 1.1127121448516846 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:06:22,410\tWARNING util.py:136 -- The `process_trial` operation took 0.7794351577758789 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:08:44,296\tWARNING util.py:136 -- The `process_trial` operation took 8.586956977844238 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:09:11,244\tWARNING util.py:136 -- The `process_trial` operation took 5.175590991973877 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:09:15,890\tWARNING util.py:136 -- The `process_trial` operation took 4.378533124923706 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:09:21,305\tWARNING util.py:136 -- The `process_trial` operation took 4.834406852722168 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:09:24,236\tWARNING util.py:136 -- The `process_trial` operation took 2.913648843765259 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:09:25,905\tWARNING util.py:136 -- The `process_trial` operation took 1.6513750553131104 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:09:28,976\tWARNING util.py:136 -- The `process_trial` operation took 2.690347909927368 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:09:52,868\tWARNING util.py:136 -- The `process_trial` operation took 3.3819830417633057 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:09:57,287\tWARNING util.py:136 -- The `process_trial` operation took 1.9341456890106201 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:10:18,173\tWARNING util.py:136 -- The `process_trial` operation took 0.5877199172973633 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:10:19,023\tWARNING util.py:136 -- The `process_trial` operation took 0.6872689723968506 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:12:21,068\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 1.6336891651153564 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:15:24,303\tWARNING util.py:136 -- The `process_trial` operation took 4.933242082595825 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:15:25,117\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff7a3160c901000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 15:15:26,217\tINFO (unknown file):0 -- gc.collect() freed 601 refs in 0.9940709779998542 seconds\n",
      "2020-10-05 15:15:43,662\tWARNING util.py:136 -- The `process_trial` operation took 4.312890291213989 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:15:50,496\tWARNING util.py:136 -- The `process_trial` operation took 4.219387769699097 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:15:59,398\tWARNING util.py:136 -- The `process_trial` operation took 3.805860996246338 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:16:18,224\tWARNING util.py:136 -- The `process_trial` operation took 3.2744622230529785 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:16:31,218\tWARNING util.py:136 -- The `process_trial` operation took 4.142214775085449 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:16:47,850\tWARNING util.py:136 -- The `process_trial` operation took 2.4089529514312744 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:17:18,739\tWARNING util.py:136 -- The `process_trial` operation took 1.473487138748169 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:17:38,234\tWARNING util.py:136 -- The `process_trial` operation took 0.5773730278015137 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:17:38,881\tWARNING util.py:136 -- The `process_trial` operation took 0.5907740592956543 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:25:11,163\tWARNING util.py:136 -- The `process_trial` operation took 5.2248570919036865 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:25:25,596\tWARNING util.py:136 -- The `process_trial` operation took 3.8583626747131348 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:25:40,313\tWARNING util.py:136 -- The `process_trial` operation took 4.919757843017578 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:25:57,445\tWARNING util.py:136 -- The `process_trial` operation took 4.307291269302368 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:26:26,669\tWARNING util.py:136 -- The `process_trial` operation took 4.4114110469818115 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:26:35,733\tWARNING util.py:136 -- The `process_trial` operation took 2.630241870880127 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:27:37,917\tWARNING util.py:136 -- The `process_trial` operation took 1.7239577770233154 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:28:36,758\tWARNING util.py:136 -- The `process_trial` operation took 1.33831787109375 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:28:41,420\tWARNING util.py:136 -- The `process_trial` operation took 1.456834077835083 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:28:57,217\tWARNING util.py:136 -- The `process_trial` operation took 0.9176981449127197 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:34:01,408\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff1c07f3f901000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 15:34:02,505\tINFO (unknown file):0 -- gc.collect() freed 221 refs in 0.9932282859999759 seconds\n",
      "2020-10-05 15:34:46,483\tWARNING util.py:136 -- The `process_trial` operation took 3.1918411254882812 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:36:14,435\tWARNING util.py:136 -- The `process_trial` operation took 3.2704789638519287 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:36:19,185\tWARNING util.py:136 -- The `process_trial` operation took 2.540079116821289 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:36:35,034\tWARNING util.py:136 -- The `process_trial` operation took 2.0047080516815186 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:37:57,842\tWARNING util.py:136 -- The `process_trial` operation took 4.618870973587036 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:38:28,085\tWARNING util.py:136 -- The `process_trial` operation took 2.1984031200408936 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:38:58,683\tWARNING util.py:136 -- The `process_trial` operation took 0.9386129379272461 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:39:18,773\tWARNING util.py:136 -- The `process_trial` operation took 1.0104119777679443 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:39:33,995\tWARNING util.py:136 -- The `process_trial` operation took 1.0318851470947266 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:39:57,893\tWARNING util.py:136 -- The `process_trial` operation took 0.7409510612487793 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:44:56,382\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 15:44:56,389\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 15:44:56,395\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 15:44:56,479\tWARNING util.py:136 -- The `process_trial` operation took 4.637168884277344 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:46:12,347\tWARNING util.py:136 -- The `process_trial` operation took 6.407210111618042 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:47:15,358\tWARNING util.py:136 -- The `process_trial` operation took 6.186336994171143 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:47:21,091\tWARNING util.py:136 -- The `process_trial` operation took 5.50292706489563 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:47:27,651\tWARNING util.py:136 -- The `process_trial` operation took 5.868611812591553 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:47:42,428\tWARNING util.py:136 -- The `process_trial` operation took 4.255111932754517 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:48:38,686\tWARNING util.py:136 -- The `process_trial` operation took 4.940323114395142 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:49:28,473\tWARNING util.py:136 -- The `process_trial` operation took 2.1345179080963135 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:49:57,146\tWARNING util.py:136 -- The `process_trial` operation took 1.8380768299102783 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:51:22,065\tWARNING util.py:136 -- The `process_trial` operation took 1.3500442504882812 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:51:42,322\tWARNING util.py:136 -- The `process_trial` operation took 0.8916571140289307 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:53:26,094\tWARNING util.py:136 -- The `process_trial` operation took 0.5276970863342285 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:55:38,762\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff66f9b18501000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 15:55:38,764\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 15:55:40,310\tWARNING util.py:136 -- The `process_trial` operation took 5.461169958114624 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:56:16,600\tWARNING util.py:136 -- The `process_trial` operation took 2.611466884613037 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:56:17,731\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff0cccf97001000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 15:56:18,772\tINFO (unknown file):0 -- gc.collect() freed 568 refs in 0.9389973069992266 seconds\n",
      "2020-10-05 15:56:25,941\tWARNING util.py:136 -- The `process_trial` operation took 3.5819199085235596 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:56:26,569\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.6255068778991699 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:56:38,622\tWARNING util.py:136 -- The `process_trial` operation took 3.1050171852111816 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:56:49,613\tWARNING util.py:136 -- The `process_trial` operation took 2.2282590866088867 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:57:12,774\tWARNING util.py:136 -- The `process_trial` operation took 5.401565074920654 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:57:35,249\tWARNING util.py:136 -- The `process_trial` operation took 1.8962368965148926 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:57:37,443\tWARNING util.py:136 -- The `process_trial` operation took 1.5514748096466064 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:57:39,615\tWARNING util.py:136 -- The `process_trial` operation took 1.2840790748596191 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:57:48,279\tWARNING util.py:136 -- The `process_trial` operation took 0.5858638286590576 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:58:27,151\tWARNING util.py:136 -- The `process_trial` operation took 0.5492079257965088 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 15:58:28,232\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff6438f92c01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 8.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 8 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 15:58:28,459\tINFO (unknown file):0 -- gc.collect() freed 275 refs in 0.13785563399960665 seconds\n",
      "2020-10-05 16:00:41,174\tWARNING util.py:136 -- The `process_trial` operation took 3.772907018661499 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:01:05,964\tWARNING util.py:136 -- The `process_trial` operation took 3.4728009700775146 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:01:09,323\tWARNING worker.py:1072 -- The actor or task with ID fffffffffffffffff3f784b401000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 16:01:12,941\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 16:01:12,946\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 16:01:12,978\tINFO (unknown file):0 -- gc.collect() freed 568 refs in 3.50938840300023 seconds\n",
      "2020-10-05 16:01:17,519\tWARNING util.py:136 -- The `process_trial` operation took 4.148688077926636 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:01:22,664\tWARNING util.py:136 -- The `process_trial` operation took 4.897989988327026 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:01:28,252\tWARNING util.py:136 -- The `process_trial` operation took 5.461029767990112 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:01:35,477\tWARNING util.py:136 -- The `process_trial` operation took 4.014333963394165 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:01:53,337\tWARNING util.py:136 -- The `process_trial` operation took 4.864562034606934 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:01:58,693\tWARNING util.py:136 -- The `process_trial` operation took 2.6754350662231445 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:02:01,238\tWARNING util.py:136 -- The `process_trial` operation took 1.541126012802124 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:02:06,913\tWARNING util.py:136 -- The `process_trial` operation took 0.9828710556030273 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:02:16,298\tWARNING util.py:136 -- The `process_trial` operation took 0.5160481929779053 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:04:50,602\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff7635a15f01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 2.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 16:04:51,728\tINFO (unknown file):0 -- gc.collect() freed 369 refs in 1.023181859000033 seconds\n",
      "2020-10-05 16:05:01,747\tWARNING util.py:136 -- The `process_trial` operation took 2.390002965927124 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:05:04,395\tWARNING util.py:136 -- The `process_trial` operation took 1.9212071895599365 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:05:12,899\tWARNING util.py:136 -- The `process_trial` operation took 1.143406867980957 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:05:33,741\tWARNING util.py:136 -- The `process_trial` operation took 3.0031187534332275 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:05:58,769\tWARNING util.py:136 -- The `process_trial` operation took 2.003159999847412 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:06:00,829\tWARNING util.py:136 -- The `process_trial` operation took 2.0301387310028076 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:06:02,546\tWARNING util.py:136 -- The `process_trial` operation took 1.1724870204925537 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:06:25,213\tWARNING util.py:136 -- The `process_trial` operation took 0.6888039112091064 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:06:32,510\tWARNING util.py:136 -- The `process_trial` operation took 0.5154769420623779 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:06:47,224\tWARNING util.py:136 -- The `process_trial` operation took 0.6180508136749268 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:09:55,428\tWARNING util.py:136 -- The `process_trial` operation took 3.1533749103546143 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:10:21,657\tWARNING util.py:136 -- The `process_trial` operation took 3.7938501834869385 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:10:22,354\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffffeef3fe8101000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.416016 GiB}, {memory: 4.150391 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 16:10:23,387\tINFO (unknown file):0 -- gc.collect() freed 568 refs in 0.9371172800001659 seconds\n",
      "2020-10-05 16:10:26,540\tWARNING util.py:136 -- The `process_trial` operation took 2.6050472259521484 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:10:35,829\tWARNING util.py:136 -- The `process_trial` operation took 3.45265793800354 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:10:39,259\tWARNING util.py:136 -- The `process_trial` operation took 2.5441079139709473 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:11:17,179\tWARNING util.py:136 -- The `process_trial` operation took 2.612794876098633 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:11:19,157\tWARNING util.py:136 -- The `process_trial` operation took 1.805516004562378 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:11:23,629\tWARNING util.py:136 -- The `process_trial` operation took 1.0327949523925781 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:11:25,202\tWARNING util.py:136 -- The `process_trial` operation took 0.939816951751709 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 16:11:48,434\tWARNING util.py:136 -- The `process_trial` operation took 0.5143768787384033 seconds to complete, which may be a performance bottleneck.\n"
     ]
    }
   ],
   "source": [
    "%%capture tf_run_output\n",
    "\n",
    "### for each space in hyperspace, we want to search the space using ray tune\n",
    "results = []\n",
    "for section in tqdm(space):\n",
    "    # create a skopt gp minimize object\n",
    "    optimizer = Optimizer(section)\n",
    "    search_algo = SkOptSearch(optimizer, ['learning_rate', 'dropout', 'epochs', 'batch_size'],\n",
    "                              metric='test_loss', mode='min')\n",
    "    # not using a gpu because running on local\n",
    "    analysis = tune.run(mnist_tf_objective, search_alg=search_algo, num_samples=20, local_dir=\"~/Documents/hyper_resilient/experiments/exp1\")\n",
    "    results.append(analysis)\n",
    "\n",
    "# # print out the best result\n",
    "# i = 0\n",
    "# for a in results:\n",
    "#     print(\"Best config for space \"+str(i)+\": \"+a.get_best_config(metric=\"test_loss\", mode=\"min\"))\n",
    "#     i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6c8f408b0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6c8fdc910>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6c8cd75e0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6c9a38340>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6ca51c4c0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6ca478520>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6ca35c7f0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6ca478b20>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6c8f07250>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6caabf400>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6c8cd7250>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6cc783be0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6ca40cc10>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6ca593220>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6ca19df40>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa6cc327d30>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tf_results = tf_results[0].results_df\n",
    "for i in range(1, len(tf_results)):\n",
    "    all_tf_results = all_tf_results.append(tf_results[i].results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tf_results.to_csv('full_tf_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Model Objective Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNet(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(784, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(config['dropout']), \n",
    "            nn.Linear(128, 10), \n",
    "            nn.Softmax())\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.config = config\n",
    "        self.test_loss = None\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'test_loss': loss}\n",
    "        return {'test_loss': loss, 'logs': logs}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = []\n",
    "        for x in outputs:\n",
    "            loss.append(float(x['test_loss']))\n",
    "        avg_loss = statistics.mean(loss)\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        self.test_loss = avg_loss\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_pt_objective(config):\n",
    "    model = NumberNet(config)\n",
    "    trainer = pl.Trainer(max_epochs=config['epochs'])\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "    tune.report(test_loss=model.test_loss)\n",
    "    return model.test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-05 01:09:10,616\tWARNING util.py:136 -- The `process_trial` operation took 0.9559600353240967 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:10:40,591\tWARNING util.py:136 -- The `process_trial` operation took 0.6296718120574951 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:13:13,709\tWARNING util.py:136 -- The `process_trial` operation took 0.6996901035308838 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:13:53,407\tWARNING util.py:136 -- The `process_trial` operation took 0.7335710525512695 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:16:11,570\tWARNING util.py:136 -- The `process_trial` operation took 0.6475241184234619 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:19:16,368\tWARNING util.py:136 -- The `process_trial` operation took 0.6921200752258301 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:19:29,851\tWARNING util.py:136 -- The `process_trial` operation took 0.5834648609161377 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:41:23,891\tWARNING util.py:136 -- The `process_trial` operation took 0.6301476955413818 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:43:22,636\tWARNING util.py:136 -- The `process_trial` operation took 0.6802699565887451 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:45:34,432\tWARNING util.py:136 -- The `process_trial` operation took 0.6728758811950684 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:46:14,567\tWARNING util.py:136 -- The `process_trial` operation took 0.7650318145751953 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:46:32,236\tWARNING util.py:136 -- The `process_trial` operation took 0.9175910949707031 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:47:20,990\tWARNING util.py:136 -- The `process_trial` operation took 0.557701826095581 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:50:26,544\tWARNING util.py:136 -- The `process_trial` operation took 0.5620720386505127 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:51:25,854\tWARNING util.py:136 -- The `process_trial` operation took 0.6182081699371338 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:10:28,424\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff5fd8ddae01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 02:10:28,782\tINFO (unknown file):0 -- gc.collect() freed 455 refs in 0.2666840870078886 seconds\n",
      "2020-10-05 02:17:00,578\tWARNING util.py:136 -- The `process_trial` operation took 0.7610578536987305 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:19:55,742\tWARNING util.py:136 -- The `process_trial` operation took 0.7539780139923096 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:20:01,279\tWARNING util.py:136 -- The `process_trial` operation took 0.76279616355896 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:21:04,109\tWARNING util.py:136 -- The `process_trial` operation took 0.7623119354248047 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:21:18,813\tWARNING util.py:136 -- The `process_trial` operation took 0.7536098957061768 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:51:24,339\tWARNING util.py:136 -- The `process_trial` operation took 0.7189769744873047 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:53:03,621\tWARNING util.py:136 -- The `process_trial` operation took 0.6780989170074463 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:55:23,504\tWARNING util.py:136 -- The `process_trial` operation took 0.8643169403076172 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:55:24,656\tWARNING util.py:136 -- The `process_trial` operation took 1.1122448444366455 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:56:02,540\tWARNING util.py:136 -- The `process_trial` operation took 0.8139240741729736 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:56:34,649\tWARNING util.py:136 -- The `process_trial` operation took 0.7158989906311035 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:59:25,802\tWARNING util.py:136 -- The `process_trial` operation took 0.6156690120697021 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:14:48,586\tWARNING util.py:136 -- The `process_trial` operation took 0.6149730682373047 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:15:24,655\tWARNING util.py:136 -- The `process_trial` operation took 0.7129542827606201 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:17:45,364\tWARNING util.py:136 -- The `process_trial` operation took 0.9590270519256592 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:19:12,140\tWARNING util.py:136 -- The `process_trial` operation took 0.6408951282501221 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:20:31,139\tWARNING util.py:136 -- The `process_trial` operation took 0.5437009334564209 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:21:08,583\tWARNING util.py:136 -- The `process_trial` operation took 0.7138350009918213 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:22:17,168\tWARNING util.py:136 -- The `process_trial` operation took 0.6628918647766113 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:22:37,511\tWARNING util.py:136 -- The `process_trial` operation took 0.532135009765625 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:22:39,212\tWARNING util.py:136 -- The `process_trial` operation took 0.5093388557434082 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:33:11,490\tWARNING util.py:136 -- The `process_trial` operation took 0.8586287498474121 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:33:30,964\tWARNING util.py:136 -- The `process_trial` operation took 0.7897601127624512 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:34:44,704\tWARNING util.py:136 -- The `process_trial` operation took 0.6401550769805908 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:36:07,636\tWARNING util.py:136 -- The `process_trial` operation took 0.7718968391418457 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:36:40,827\tWARNING util.py:136 -- The `process_trial` operation took 0.6318328380584717 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:37:11,285\tWARNING util.py:136 -- The `process_trial` operation took 0.6131870746612549 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:37:36,256\tWARNING util.py:136 -- The `process_trial` operation took 0.5991442203521729 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:48:42,984\tWARNING util.py:136 -- The `process_trial` operation took 0.9803042411804199 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:50:56,518\tWARNING util.py:136 -- The `process_trial` operation took 0.6596119403839111 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:52:43,400\tWARNING util.py:136 -- The `process_trial` operation took 0.7461938858032227 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:52:46,659\tWARNING util.py:136 -- The `process_trial` operation took 0.7239999771118164 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:52:55,213\tWARNING util.py:136 -- The `process_trial` operation took 0.7472779750823975 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:54:03,443\tWARNING util.py:136 -- The `process_trial` operation took 0.6486539840698242 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:54:04,203\tWARNING util.py:136 -- The `process_trial` operation took 0.7012851238250732 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:54:39,187\tWARNING util.py:136 -- The `process_trial` operation took 0.535412073135376 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:09:36,566\tWARNING util.py:136 -- The `process_trial` operation took 0.6243619918823242 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:12:17,919\tWARNING util.py:136 -- The `process_trial` operation took 0.7223358154296875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:12:30,050\tWARNING util.py:136 -- The `process_trial` operation took 0.8752808570861816 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:12:40,880\tWARNING util.py:136 -- The `process_trial` operation took 0.8701686859130859 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:12:51,753\tWARNING util.py:136 -- The `process_trial` operation took 0.7167370319366455 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:13:32,144\tWARNING util.py:136 -- The `process_trial` operation took 0.9056692123413086 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:13:40,159\tWARNING util.py:136 -- The `process_trial` operation took 0.6699938774108887 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:13:45,190\tWARNING util.py:136 -- The `process_trial` operation took 0.6128678321838379 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:14:19,644\tWARNING util.py:136 -- The `process_trial` operation took 0.5417137145996094 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:35:01,848\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff0cb5f87f01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 04:35:02,320\tINFO (unknown file):0 -- gc.collect() freed 434 refs in 0.3666324420046294 seconds\n",
      "2020-10-05 04:38:43,132\tINFO (unknown file):0 -- gc.collect() freed 137 refs in 0.3556398209912004 seconds\n",
      "2020-10-05 04:41:14,852\tWARNING util.py:136 -- The `process_trial` operation took 0.7783718109130859 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:46:24,184\tWARNING util.py:136 -- The `process_trial` operation took 0.7167739868164062 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:46:25,022\tINFO (unknown file):0 -- gc.collect() freed 578 refs in 0.28180963201157283 seconds\n",
      "2020-10-05 04:46:32,319\tWARNING util.py:136 -- The `process_trial` operation took 0.699120044708252 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:48:20,170\tWARNING util.py:136 -- The `process_trial` operation took 0.6399269104003906 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:49:52,374\tWARNING util.py:136 -- The `process_trial` operation took 0.8000380992889404 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:49:54,957\tWARNING util.py:136 -- The `process_trial` operation took 0.6914317607879639 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:50:45,114\tWARNING util.py:136 -- The `process_trial` operation took 0.7903120517730713 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:53:49,102\tWARNING util.py:136 -- The `process_trial` operation took 0.6347529888153076 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:53:53,995\tWARNING util.py:136 -- The `process_trial` operation took 0.501197099685669 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:19:20,506\tWARNING util.py:136 -- The `process_trial` operation took 0.6543080806732178 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:19:32,056\tWARNING util.py:136 -- The `process_trial` operation took 0.780785083770752 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:22:24,110\tWARNING util.py:136 -- The `process_trial` operation took 0.8712069988250732 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:22:25,211\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff3a208c5701000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 05:22:25,641\tINFO (unknown file):0 -- gc.collect() freed 771 refs in 0.33821831599925645 seconds\n",
      "2020-10-05 05:28:30,069\tWARNING util.py:136 -- The `process_trial` operation took 0.841484785079956 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:29:17,150\tWARNING util.py:136 -- The `process_trial` operation took 0.8014373779296875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:30:30,951\tWARNING util.py:136 -- The `process_trial` operation took 0.5637550354003906 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:31:44,677\tWARNING util.py:136 -- The `process_trial` operation took 0.6907660961151123 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:33:10,746\tWARNING util.py:136 -- The `process_trial` operation took 0.5810654163360596 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:33:39,907\tWARNING util.py:136 -- The `process_trial` operation took 0.5638651847839355 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:09:54,358\tWARNING util.py:136 -- The `process_trial` operation took 0.8675358295440674 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:10:28,136\tWARNING util.py:136 -- The `process_trial` operation took 0.7722170352935791 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:11:55,600\tWARNING util.py:136 -- The `process_trial` operation took 0.6198468208312988 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:12:05,434\tWARNING util.py:136 -- The `process_trial` operation took 0.721405029296875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:12:24,239\tWARNING util.py:136 -- The `process_trial` operation took 0.7680249214172363 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:12:41,694\tWARNING util.py:136 -- The `process_trial` operation took 0.6846201419830322 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:13:16,391\tWARNING util.py:136 -- The `process_trial` operation took 0.6668210029602051 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:15:56,662\tWARNING util.py:136 -- The `process_trial` operation took 1.0098509788513184 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:51:46,740\tWARNING util.py:136 -- The `process_trial` operation took 0.7648310661315918 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:52:53,787\tWARNING util.py:136 -- The `process_trial` operation took 0.5007281303405762 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:53:03,351\tWARNING util.py:136 -- The `process_trial` operation took 1.2367639541625977 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:53:13,920\tWARNING util.py:136 -- The `process_trial` operation took 0.7295670509338379 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:53:40,419\tWARNING util.py:136 -- The `process_trial` operation took 0.8262560367584229 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:56:50,346\tWARNING util.py:136 -- The `process_trial` operation took 0.5999209880828857 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:56:55,952\tWARNING util.py:136 -- The `process_trial` operation took 0.5856640338897705 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:56:59,302\tWARNING util.py:136 -- The `process_trial` operation took 0.5626950263977051 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:59:38,315\tWARNING util.py:136 -- The `process_trial` operation took 0.5918469429016113 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:18:19,413\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff130030fa01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 07:18:19,857\tINFO (unknown file):0 -- gc.collect() freed 578 refs in 0.3425632010039408 seconds\n",
      "2020-10-05 07:18:33,385\tWARNING util.py:136 -- The `process_trial` operation took 1.0066907405853271 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:18:36,535\tWARNING util.py:136 -- The `process_trial` operation took 1.102480173110962 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:22:06,447\tWARNING util.py:136 -- The `process_trial` operation took 0.6812667846679688 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:22:08,437\tWARNING util.py:136 -- The `process_trial` operation took 1.116215705871582 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:23:20,569\tWARNING util.py:136 -- The `process_trial` operation took 0.7805089950561523 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:23:35,896\tWARNING util.py:136 -- The `process_trial` operation took 0.8241617679595947 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:24:02,374\tWARNING util.py:136 -- The `process_trial` operation took 0.7201240062713623 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:25:55,569\tWARNING util.py:136 -- The `process_trial` operation took 0.6736791133880615 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:28:38,430\tWARNING util.py:136 -- The `process_trial` operation took 0.619905948638916 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:34:03,293\tWARNING util.py:136 -- The `process_trial` operation took 0.5171501636505127 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:47:28,331\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff937342cd01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 07:47:28,802\tINFO (unknown file):0 -- gc.collect() freed 401 refs in 0.3670527409994975 seconds\n",
      "2020-10-05 07:49:11,719\tWARNING util.py:136 -- The `process_trial` operation took 0.8393349647521973 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:49:32,100\tWARNING util.py:136 -- The `process_trial` operation took 0.7972688674926758 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:52:23,378\tWARNING util.py:136 -- The `process_trial` operation took 0.9997148513793945 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:53:16,625\tWARNING util.py:136 -- The `process_trial` operation took 0.8392829895019531 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:53:59,406\tWARNING util.py:136 -- The `process_trial` operation took 0.7867043018341064 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:54:06,122\tWARNING util.py:136 -- The `process_trial` operation took 0.8255581855773926 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:55:03,341\tWARNING util.py:136 -- The `process_trial` operation took 0.7078230381011963 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:55:57,459\tWARNING util.py:136 -- The `process_trial` operation took 0.6324379444122314 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:56:36,342\tWARNING util.py:136 -- The `process_trial` operation took 0.5371367931365967 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:15:57,227\tWARNING util.py:136 -- The `process_trial` operation took 0.7875690460205078 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:16:02,146\tWARNING util.py:136 -- The `process_trial` operation took 0.8702259063720703 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:16:36,165\tWARNING util.py:136 -- The `process_trial` operation took 0.7490520477294922 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:16:37,015\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff102d304001000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 08:16:37,472\tINFO (unknown file):0 -- gc.collect() freed 771 refs in 0.36160219201701693 seconds\n",
      "2020-10-05 08:16:41,351\tWARNING util.py:136 -- The `process_trial` operation took 0.7966089248657227 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:16:50,153\tWARNING util.py:136 -- The `process_trial` operation took 0.8274497985839844 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:19:05,432\tWARNING util.py:136 -- The `process_trial` operation took 0.8022730350494385 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:19:35,379\tWARNING util.py:136 -- The `process_trial` operation took 0.7854599952697754 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:20:51,941\tWARNING util.py:136 -- The `process_trial` operation took 0.5797548294067383 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:21:03,578\tWARNING util.py:136 -- The `process_trial` operation took 0.5127899646759033 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:21:34,671\tWARNING util.py:136 -- The `process_trial` operation took 0.5252151489257812 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:35:56,032\tWARNING util.py:136 -- The `process_trial` operation took 0.6913552284240723 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:38:28,825\tWARNING util.py:136 -- The `process_trial` operation took 0.6818523406982422 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:38:45,495\tWARNING util.py:136 -- The `process_trial` operation took 0.7610533237457275 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:39:52,357\tWARNING util.py:136 -- The `process_trial` operation took 0.5770130157470703 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:40:33,309\tWARNING util.py:136 -- The `process_trial` operation took 0.5916500091552734 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:42:55,115\tWARNING util.py:136 -- The `process_trial` operation took 0.5290818214416504 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:43:05,435\tWARNING util.py:136 -- The `process_trial` operation took 0.5578219890594482 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:43:28,530\tWARNING util.py:136 -- The `process_trial` operation took 0.6276211738586426 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:46:13,880\tWARNING experiment_analysis.py:533 -- Could not find best trial. Did you pass the correct `metric`parameter?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-75ac5b7c8f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best config for space \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"avg_test_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "%%capture pt_run_output\n",
    "# hyperparameters = [(0.00000001, 0.1),  # learning_rate\n",
    "#                    (0.0, 0.9),  # dropout\n",
    "#                    (10, 100),  # epochs \n",
    "#                    (10, 1000)]  # batch size\n",
    "# space = create_hyperspace(hyperparameters)\n",
    "\n",
    "### for each space in hyperspace, we want to search the space using ray tune\n",
    "\n",
    "results = []\n",
    "for section in tqdm(space):\n",
    "    # create a skopt gp minimize object\n",
    "    optimizer = Optimizer(section)\n",
    "    search_algo = SkOptSearch(optimizer, ['learning_rate', 'dropout', 'epochs', 'batch_size'],\n",
    "                              metric='test_loss', mode='min')\n",
    "    # not using a gpu because running on local\n",
    "    analysis = tune.run(mnist_pt_objective, search_alg=search_algo, num_samples=20)\n",
    "    results.append(analysis)\n",
    "\n",
    "# print out the best result\n",
    "# i = 0\n",
    "# for a in results:\n",
    "#     print(\"Best config for space \"+str(i)+\": \"+a.get_best_config(metric=\"avg_test_loss\", mode=\"min\"))\n",
    "#     i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pt_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-191bd6ce2a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpt_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pt_results' is not defined"
     ]
    }
   ],
   "source": [
    "pt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_results = pt_results[0].results_df\n",
    "for i in range(1, len(pt_results)):\n",
    "    all_pt_results = all_pt_results.append(pt_results[i].results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>...</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>timesteps_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>experiment_tag</th>\n",
       "      <th>config.learning_rate</th>\n",
       "      <th>config.dropout</th>\n",
       "      <th>config.epochs</th>\n",
       "      <th>config.batch_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3f34fb72</th>\n",
       "      <td>1.560013</td>\n",
       "      <td>758.230129</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>b6eced49168f4073923784cbd55b2930</td>\n",
       "      <td>2020-10-05_01-01-53</td>\n",
       "      <td>1601884913</td>\n",
       "      <td>758.230129</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>758.230129</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1_batch_size=668,dropout=0.62961,epochs=73,lea...</td>\n",
       "      <td>0.054076</td>\n",
       "      <td>0.629609</td>\n",
       "      <td>73</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f3549d8</th>\n",
       "      <td>1.570882</td>\n",
       "      <td>942.381004</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>ee4347a8603246e189280fe2e0f6bfb5</td>\n",
       "      <td>2020-10-05_01-04-57</td>\n",
       "      <td>1601885097</td>\n",
       "      <td>942.381004</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>942.381004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2_batch_size=730,dropout=0.51978,epochs=91,lea...</td>\n",
       "      <td>0.068914</td>\n",
       "      <td>0.519781</td>\n",
       "      <td>91</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f3588b2</th>\n",
       "      <td>1.597828</td>\n",
       "      <td>864.161679</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>f6dbe4679d024e828d99cb25e81af958</td>\n",
       "      <td>2020-10-05_01-03-39</td>\n",
       "      <td>1601885019</td>\n",
       "      <td>864.161679</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>864.161679</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3_batch_size=837,dropout=0.62676,epochs=84,lea...</td>\n",
       "      <td>0.079474</td>\n",
       "      <td>0.626760</td>\n",
       "      <td>84</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f35c926</th>\n",
       "      <td>1.621406</td>\n",
       "      <td>894.386705</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>d057cfd110154abfa0581e77cd8c28f7</td>\n",
       "      <td>2020-10-05_01-04-09</td>\n",
       "      <td>1601885049</td>\n",
       "      <td>894.386705</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>894.386705</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4_batch_size=841,dropout=0.53659,epochs=87,lea...</td>\n",
       "      <td>0.094646</td>\n",
       "      <td>0.536590</td>\n",
       "      <td>87</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f360706</th>\n",
       "      <td>1.650161</td>\n",
       "      <td>517.897673</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>f60e033dd9854d629ae64912ab9896af</td>\n",
       "      <td>2020-10-05_00-57-54</td>\n",
       "      <td>1601884674</td>\n",
       "      <td>517.897673</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>517.897673</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5_batch_size=946,dropout=0.74561,epochs=50,lea...</td>\n",
       "      <td>0.097394</td>\n",
       "      <td>0.745606</td>\n",
       "      <td>50</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88a6f3de</th>\n",
       "      <td>1.501839</td>\n",
       "      <td>395.798915</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>307cd180a1dd4f768fedabff9d98c876</td>\n",
       "      <td>2020-10-05_08-40-32</td>\n",
       "      <td>1601912432</td>\n",
       "      <td>395.798915</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>395.798915</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16_batch_size=479,dropout=0.00095588,epochs=34...</td>\n",
       "      <td>0.027286</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>34</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88a74afa</th>\n",
       "      <td>1.918050</td>\n",
       "      <td>351.211188</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>7e885715c9f24e919920a6fd2ff0aa13</td>\n",
       "      <td>2020-10-05_08-39-51</td>\n",
       "      <td>1601912391</td>\n",
       "      <td>351.211188</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>351.211188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17_batch_size=29,dropout=0.30593,epochs=16,lea...</td>\n",
       "      <td>0.042727</td>\n",
       "      <td>0.305934</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88a7a1b2</th>\n",
       "      <td>1.507322</td>\n",
       "      <td>477.634328</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>b8081cbbdffa4f12aeb42c6d57fc313e</td>\n",
       "      <td>2020-10-05_08-44-00</td>\n",
       "      <td>1601912640</td>\n",
       "      <td>477.634328</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>477.634328</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18_batch_size=458,dropout=0.4681,epochs=52,lea...</td>\n",
       "      <td>0.019268</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>52</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88a7fd88</th>\n",
       "      <td>1.555737</td>\n",
       "      <td>457.732046</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>c86f3d90bb574335b2d922813c7b2505</td>\n",
       "      <td>2020-10-05_08-46-13</td>\n",
       "      <td>1601912773</td>\n",
       "      <td>457.732046</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>457.732046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19_batch_size=293,dropout=0.29638,epochs=64,le...</td>\n",
       "      <td>0.033494</td>\n",
       "      <td>0.296376</td>\n",
       "      <td>64</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88a87074</th>\n",
       "      <td>1.539169</td>\n",
       "      <td>275.546263</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>56d23e315a4345beb18445c913511ef9</td>\n",
       "      <td>2020-10-05_08-43-27</td>\n",
       "      <td>1601912607</td>\n",
       "      <td>275.546263</td>\n",
       "      <td>...</td>\n",
       "      <td>CSI0354806</td>\n",
       "      <td>192.168.1.240</td>\n",
       "      <td>275.546263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20_batch_size=115,dropout=0.1922,epochs=26,lea...</td>\n",
       "      <td>0.020490</td>\n",
       "      <td>0.192197</td>\n",
       "      <td>26</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          test_loss  time_this_iter_s  done timesteps_total episodes_total  \\\n",
       "trial_id                                                                     \n",
       "3f34fb72   1.560013        758.230129  True            None           None   \n",
       "3f3549d8   1.570882        942.381004  True            None           None   \n",
       "3f3588b2   1.597828        864.161679  True            None           None   \n",
       "3f35c926   1.621406        894.386705  True            None           None   \n",
       "3f360706   1.650161        517.897673  True            None           None   \n",
       "...             ...               ...   ...             ...            ...   \n",
       "88a6f3de   1.501839        395.798915  True            None           None   \n",
       "88a74afa   1.918050        351.211188  True            None           None   \n",
       "88a7a1b2   1.507322        477.634328  True            None           None   \n",
       "88a7fd88   1.555737        457.732046  True            None           None   \n",
       "88a87074   1.539169        275.546263  True            None           None   \n",
       "\n",
       "          training_iteration                     experiment_id  \\\n",
       "trial_id                                                         \n",
       "3f34fb72                   1  b6eced49168f4073923784cbd55b2930   \n",
       "3f3549d8                   1  ee4347a8603246e189280fe2e0f6bfb5   \n",
       "3f3588b2                   1  f6dbe4679d024e828d99cb25e81af958   \n",
       "3f35c926                   1  d057cfd110154abfa0581e77cd8c28f7   \n",
       "3f360706                   1  f60e033dd9854d629ae64912ab9896af   \n",
       "...                      ...                               ...   \n",
       "88a6f3de                   1  307cd180a1dd4f768fedabff9d98c876   \n",
       "88a74afa                   1  7e885715c9f24e919920a6fd2ff0aa13   \n",
       "88a7a1b2                   1  b8081cbbdffa4f12aeb42c6d57fc313e   \n",
       "88a7fd88                   1  c86f3d90bb574335b2d922813c7b2505   \n",
       "88a87074                   1  56d23e315a4345beb18445c913511ef9   \n",
       "\n",
       "                         date   timestamp  time_total_s  ...    hostname  \\\n",
       "trial_id                                                 ...               \n",
       "3f34fb72  2020-10-05_01-01-53  1601884913    758.230129  ...  CSI0354806   \n",
       "3f3549d8  2020-10-05_01-04-57  1601885097    942.381004  ...  CSI0354806   \n",
       "3f3588b2  2020-10-05_01-03-39  1601885019    864.161679  ...  CSI0354806   \n",
       "3f35c926  2020-10-05_01-04-09  1601885049    894.386705  ...  CSI0354806   \n",
       "3f360706  2020-10-05_00-57-54  1601884674    517.897673  ...  CSI0354806   \n",
       "...                       ...         ...           ...  ...         ...   \n",
       "88a6f3de  2020-10-05_08-40-32  1601912432    395.798915  ...  CSI0354806   \n",
       "88a74afa  2020-10-05_08-39-51  1601912391    351.211188  ...  CSI0354806   \n",
       "88a7a1b2  2020-10-05_08-44-00  1601912640    477.634328  ...  CSI0354806   \n",
       "88a7fd88  2020-10-05_08-46-13  1601912773    457.732046  ...  CSI0354806   \n",
       "88a87074  2020-10-05_08-43-27  1601912607    275.546263  ...  CSI0354806   \n",
       "\n",
       "                node_ip time_since_restore  timesteps_since_restore  \\\n",
       "trial_id                                                              \n",
       "3f34fb72  192.168.1.240         758.230129                        0   \n",
       "3f3549d8  192.168.1.240         942.381004                        0   \n",
       "3f3588b2  192.168.1.240         864.161679                        0   \n",
       "3f35c926  192.168.1.240         894.386705                        0   \n",
       "3f360706  192.168.1.240         517.897673                        0   \n",
       "...                 ...                ...                      ...   \n",
       "88a6f3de  192.168.1.240         395.798915                        0   \n",
       "88a74afa  192.168.1.240         351.211188                        0   \n",
       "88a7a1b2  192.168.1.240         477.634328                        0   \n",
       "88a7fd88  192.168.1.240         457.732046                        0   \n",
       "88a87074  192.168.1.240         275.546263                        0   \n",
       "\n",
       "          iterations_since_restore  \\\n",
       "trial_id                             \n",
       "3f34fb72                         1   \n",
       "3f3549d8                         1   \n",
       "3f3588b2                         1   \n",
       "3f35c926                         1   \n",
       "3f360706                         1   \n",
       "...                            ...   \n",
       "88a6f3de                         1   \n",
       "88a74afa                         1   \n",
       "88a7a1b2                         1   \n",
       "88a7fd88                         1   \n",
       "88a87074                         1   \n",
       "\n",
       "                                             experiment_tag  \\\n",
       "trial_id                                                      \n",
       "3f34fb72  1_batch_size=668,dropout=0.62961,epochs=73,lea...   \n",
       "3f3549d8  2_batch_size=730,dropout=0.51978,epochs=91,lea...   \n",
       "3f3588b2  3_batch_size=837,dropout=0.62676,epochs=84,lea...   \n",
       "3f35c926  4_batch_size=841,dropout=0.53659,epochs=87,lea...   \n",
       "3f360706  5_batch_size=946,dropout=0.74561,epochs=50,lea...   \n",
       "...                                                     ...   \n",
       "88a6f3de  16_batch_size=479,dropout=0.00095588,epochs=34...   \n",
       "88a74afa  17_batch_size=29,dropout=0.30593,epochs=16,lea...   \n",
       "88a7a1b2  18_batch_size=458,dropout=0.4681,epochs=52,lea...   \n",
       "88a7fd88  19_batch_size=293,dropout=0.29638,epochs=64,le...   \n",
       "88a87074  20_batch_size=115,dropout=0.1922,epochs=26,lea...   \n",
       "\n",
       "         config.learning_rate  config.dropout  config.epochs  \\\n",
       "trial_id                                                       \n",
       "3f34fb72             0.054076        0.629609             73   \n",
       "3f3549d8             0.068914        0.519781             91   \n",
       "3f3588b2             0.079474        0.626760             84   \n",
       "3f35c926             0.094646        0.536590             87   \n",
       "3f360706             0.097394        0.745606             50   \n",
       "...                       ...             ...            ...   \n",
       "88a6f3de             0.027286        0.000956             34   \n",
       "88a74afa             0.042727        0.305934             16   \n",
       "88a7a1b2             0.019268        0.468100             52   \n",
       "88a7fd88             0.033494        0.296376             64   \n",
       "88a87074             0.020490        0.192197             26   \n",
       "\n",
       "          config.batch_size  \n",
       "trial_id                     \n",
       "3f34fb72                668  \n",
       "3f3549d8                730  \n",
       "3f3588b2                837  \n",
       "3f35c926                841  \n",
       "3f360706                946  \n",
       "...                     ...  \n",
       "88a6f3de                479  \n",
       "88a74afa                 29  \n",
       "88a7a1b2                458  \n",
       "88a7fd88                293  \n",
       "88a87074                115  \n",
       "\n",
       "[320 rows x 21 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_pt_results = all_pt_results[['config.learning_rate','config.dropout', 'config.epochs', 'config.batch_size', 'test_loss']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
